{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWwUBGkvsWAm"
   },
   "source": [
    "## Lab 10. Face detection\n",
    "\n",
    "## Lab tasks\n",
    "\n",
    "#### Apply Viola Jones Algorithm to detect human faces in the image. The result should look like this:\n",
    "![](https://i.imgur.com/r9wLclq.png)\n",
    "\n",
    "#### Apply Viola Jones Algorithm to detect cat faces in the image. The result should look like this:\n",
    "![](https://i.imgur.com/nSLRiKb.png)\n",
    "\n",
    "#### Replace human faces with cat faces:\n",
    "![](https://i.imgur.com/SIRQi6M.png)\n",
    "\n",
    "#### Apply deep learning to face detection:\n",
    "![](https://i.imgur.com/qQZoXvN.png)\n",
    "\n",
    "#### Apply any of these algorithms to detect faces on each frame of the video and if there are more than two faces swap them between each other. You will get something like this but on a video:\n",
    "![](https://i.imgur.com/shTuXvS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiKxR4dsIW9o"
   },
   "source": [
    "\n",
    "## Algorithms for face detection\n",
    "\n",
    "### Viola Jones using OpenCV\n",
    "```\n",
    "# use haarcascade_frontalface_default.xml to detect people's faces\n",
    "# use haarcascade_frontalcatface.xml to detect cat's faces\n",
    "\n",
    "  face_cascade_pulp = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "  print(face_cascade_pulp)\n",
    "  start = time.time()\n",
    "  faces = face_cascade_pulp.detectMultiScale(img_pulp)\n",
    "  print(f'time taken: {time.time()-start}')\n",
    "  print(len(faces))\n",
    "  for (x,y,w,h) in faces:\n",
    "      cv2.rectangle(img_pulp_copy,(x,y),(x+w,y+h),(255,0,0),4)\n",
    "          \n",
    "  showInRow([img_pulp_copy])\n",
    "```\n",
    "Viola Jones explained: [link](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html)\n",
    "\n",
    "### Face Detection in OpenCV Using Deep Learning\n",
    "Using a pretrained neural network\n",
    "```\n",
    "  net = cv2.dnn.readNetFromCaffe('deploy.prototxt.txt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "  confidence = 0.5\n",
    "\n",
    "  (h, w) = image.shape[:2]\n",
    "  blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0), swapRB = False)\n",
    "      \n",
    "  net.setInput(blob)\n",
    "  detections = net.forward()\n",
    "\n",
    "  # filter out weak detections\n",
    "  for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "      cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5LQytumiJIo"
   },
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4VuUbxJiLuG"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10) # (w, h)\n",
    "import time, cv2, math\n",
    "from typing import AnyStr, Any, Callable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PS0TlTAUkyy"
   },
   "source": [
    "### Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZyRRoFpUnhn",
    "outputId": "5dcddb69-451d-4d64-8b92-574c5881b710"
   },
   "outputs": [],
   "source": [
    "!wget https://avatars.mds.yandex.net/get-kinopoisk-post-img/1642096/6a983456f20bd5c47db18643146cd8e7/960x540 -O pulp.jpg\n",
    "!wget https://vignette.wikia.nocookie.net/houseofnight/images/8/8b/Cats.jpg/revision/latest?cb=20130812053537 -O cats.jpg\n",
    "!wget https://github.com/RufinaMay/CV2019Fall_Pictures/raw/d0c95c6b3ed54dbb1c6eb7117a8202357617af24/okgo2.mp4?raw=true -O vid.mp4\n",
    "clear_output()\n",
    "print('Download completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2FvEyaciBHo"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZddgSWziD-H"
   },
   "outputs": [],
   "source": [
    "# def read_and_resize_image(filename, grayscale = False, fx= 0.5, fy=0.5):\n",
    "#   if grayscale:\n",
    "#     img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "#   else:\n",
    "#     imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "#     img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
    "#   img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
    "#   return img_result\n",
    "\n",
    "\n",
    "def read_and_resize(filename: str, grayscale: bool = False, fx: float = 1.0, fy: float = 1.0):\n",
    "    if grayscale:\n",
    "      img_result = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "      imgbgr = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "      # convert to rgb\n",
    "      img_result = cv2.cvtColor(imgbgr, cv2.COLOR_BGR2RGB)\n",
    "    # resize\n",
    "    if fx != 1.0 and fy != 1.0:\n",
    "      img_result = cv2.resize(img_result, None, fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
    "    return img_result\n",
    "\n",
    "\n",
    "def showInRow(list_of_images, titles = None, disable_ticks = False):\n",
    "  count = len(list_of_images)\n",
    "  for idx in range(count):\n",
    "    subplot = plt.subplot(1, count, idx+1)\n",
    "    if titles is not None:\n",
    "      subplot.set_title(titles[idx])\n",
    "\n",
    "    img = list_of_images[idx]\n",
    "    cmap = 'gray' if (len(img.shape) == 2 or img.shape[2] == 1) else None\n",
    "    subplot.imshow(img, cmap=cmap)\n",
    "    if disable_ticks:\n",
    "      plt.xticks([]), plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def process_video(video_path, frame_process):\n",
    "  vid = cv2.VideoCapture(video_path)\n",
    "  try:\n",
    "    while(True):\n",
    "      ret, frame = vid.read()\n",
    "      if not ret:\n",
    "        vid.release()\n",
    "        break\n",
    "\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      print(frame_process)\n",
    "      if frame_process is not None:\n",
    "        print(\"processing\")\n",
    "        frame = frame_process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "    vid.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zezPFGUvUsQY"
   },
   "source": [
    "## Perform Viola Jones algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrULSXLkgfP8",
    "outputId": "0df496f0-eec6-416a-d67b-1279df80a615"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml\n",
    "clear_output()\n",
    "print('Download completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGSBsoBbF1jV"
   },
   "source": [
    "## Open and display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "UtP0w8fnh7eu",
    "outputId": "1b3b8ace-68b8-4872-b8c3-ffa46242f2d9"
   },
   "outputs": [],
   "source": [
    "img_pulp = read_and_resize('pulp.jpg', grayscale = False, fx=1, fy=1)\n",
    "showInRow([img_pulp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3-BjcUiF9ah"
   },
   "source": [
    "## Find faces and draw rectangles around them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "ZmvWiuYhUzR-",
    "outputId": "c1fc2215-47e9-4cd9-bc95-85c293d6a834"
   },
   "outputs": [],
   "source": [
    "img_pulp_copy = img_pulp.copy()\n",
    "\n",
    "\n",
    "face_cascade_pulp = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "print(face_cascade_pulp)\n",
    "start = time.time()\n",
    "faces = face_cascade_pulp.detectMultiScale(img_pulp)\n",
    "print(f'time taken: {time.time()-start}')\n",
    "print(len(faces))\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img_pulp_copy,(x,y),(x+w,y+h),(0,0,255),4)\n",
    "\n",
    "showInRow([img_pulp_copy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdK10LDvpbAc"
   },
   "source": [
    "## Same for cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Kgt71XT-pdkp",
    "outputId": "9c995255-5126-4b01-b9a0-de7f1a80e321"
   },
   "outputs": [],
   "source": [
    "img_cats = read_and_resize('cats.jpg', grayscale = False, fx= 0.5, fy=0.5)\n",
    "showInRow([img_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-2HshIjMrBGu",
    "outputId": "5084a908-67c1-49c3-be6b-6526f7f15332"
   },
   "outputs": [],
   "source": [
    "face_cascade_cats = cv2.CascadeClassifier('haarcascade_frontalcatface.xml')\n",
    "\n",
    "cat_faces_positions = face_cascade_cats.detectMultiScale(img_cats)\n",
    "print(len(cat_faces_positions))\n",
    "cat_faces = []\n",
    "for (x,y,w,h) in cat_faces_positions:\n",
    "  cat_faces.append(img_cats.copy()[y:y+h, x:x+w])\n",
    "  cv2.rectangle(img_cats,(x,y),(x+w,y+h),(255,0,0),4)\n",
    "showInRow([img_cats])\n",
    "showInRow(cat_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5OI4liGuP3e"
   },
   "source": [
    "## Swap human faces with cat faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MX0I6TufuYc-",
    "outputId": "4096fcb2-ecfe-4ae4-82fa-0128a9ed4080"
   },
   "outputs": [],
   "source": [
    "img_copy_pulp = img_pulp.copy()\n",
    "i=0\n",
    "for (x,y,w,h) in faces:\n",
    "    fs = cv2.resize(cat_faces[i],(w,h))\n",
    "    img_copy_pulp[y:y+h, x:x+w] = fs\n",
    "    i+=1\n",
    "\n",
    "showInRow([img_copy_pulp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYQKieedUznw"
   },
   "source": [
    "## Perform Neural Network Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hb04LoK5aak"
   },
   "outputs": [],
   "source": [
    "# Upload weights and parameters\n",
    "\n",
    "!gdown 1pOilaivGeUTE5mCxDZm0rEwXp0aAS494\n",
    "!gdown 16jQB-lRs32cxFN5URulhOd1_3qc3F3UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZVLySUfKsNU"
   },
   "outputs": [],
   "source": [
    "class face_detector():\n",
    "  def __init__(self):\n",
    "    self.net = cv2.dnn.readNetFromCaffe('deploy.prototxt.txt', '/content/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    self.confidence = 0.5\n",
    "\n",
    "  def forward(self, image):\n",
    "    (self.h, self.w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0), swapRB = False)\n",
    "\n",
    "    self.net.setInput(blob)\n",
    "    self.detections = self.net.forward()\n",
    "\n",
    "  def detect(self, image):\n",
    "    self.forward(image)\n",
    "    for i in range(self.detections.shape[2]):\n",
    "      confidence = self.detections[0, 0, i, 2]\n",
    "\n",
    "      if confidence > 0.5:\n",
    "        box = self.detections[0, 0, i, 3:7] * np.array([self.w, self.h, self.w, self.h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jZLga8DNczu"
   },
   "outputs": [],
   "source": [
    "img_pulp = read_and_resize_image('pulp.jpg', grayscale = False, fx= 1, fy=1)\n",
    "\n",
    "FD = face_detector()\n",
    "\n",
    "showInRow([FD.detect(img_pulp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNV9e6i8HXDg"
   },
   "source": [
    "## Face detection on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRrRM0MiEOTF"
   },
   "outputs": [],
   "source": [
    "class ViolaJones():\n",
    "  def __init__(self):\n",
    "    self.face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "  def detect(self, image):\n",
    "    img_copy = image.copy()\n",
    "    # gray = cv2.cvtColor(img_copy, cv2.COLOR_RGB2GRAY)\n",
    "    # Detect faces\n",
    "    faces = self.face_cascade.detectMultiScale(img_copy, scaleFactor=1.0, minNeighbors=1, minSize=(10, 10))\n",
    "    # faces = self.face_cascade.detectMultiScale(image)\n",
    "    print(len(faces))\n",
    "    if len(faces)>0:\n",
    "      for (x,y,w,h) in faces:\n",
    "        img_copy = cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,0),4)\n",
    "        print('ffff')\n",
    "        print(x,y,w,h)\n",
    "        print(';;;')\n",
    "\n",
    "    return img_copy\n",
    "#       cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-etFn5IJulq"
   },
   "outputs": [],
   "source": [
    "# FD = face_detector()\n",
    "VJ = ViolaJones()\n",
    "\n",
    "def detect_faces(frame):\n",
    "  return VJ.detect(frame)\n",
    "  # return FD.detect(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41hVR6eJzbNF"
   },
   "outputs": [],
   "source": [
    "# !gdown 1Wph24YR8r2fOm0oSb42SvK3_mosHf2IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-UOvS9RXKd74",
    "outputId": "91066562-425f-42a7-e4f0-759e3c45f58c"
   },
   "outputs": [],
   "source": [
    "frame_width,frame_height = 1280, 720\n",
    "out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 20, (frame_height, frame_width))\n",
    "\n",
    "# You have to define a function detect_faces(frame) that takes a frame, does the processing and returns the output frame!\n",
    "# process_video(\"vid.mp4\", detect_faces)\n",
    "vid = cv2.VideoCapture(\"vid.mp4\")\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "try:\n",
    "  while(True):\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "      vid.release()\n",
    "      break\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (0,0), fx=1.0, fy=1.0, interpolation = cv2.INTER_LINEAR)\n",
    "    img_copy = frame.copy()\n",
    "    faces = face_cascade.detectMultiScale(img_copy)\n",
    "    print(len(faces))\n",
    "    if len(faces)>0:\n",
    "      for (x,y,w,h) in faces:\n",
    "        img_copy = cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,0),4)\n",
    "        print(x,y,w,h)\n",
    "    showInRow([img_copy])\n",
    "    frame = cv2.cvtColor(img_copy, cv2.COLOR_RGB2BGR)\n",
    "    out.write(frame)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  vid.release()\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmUCdnXKqww0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNe21aAyq5w5"
   },
   "source": [
    "### **Hough Transform**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lRxEFdDxKeb"
   },
   "source": [
    "## HoughLines\n",
    "\n",
    "**cv2.HoughLines(**)\n",
    "returns an array of (rho, theta) values. rho is measured in pixels and theta is measured in radians.\n",
    "\n",
    "First parameter, Input image should be a binary image (so apply threshold or use canny edge detection before finding applying hough transform).\n",
    "\n",
    "Second and third parameters are rho and theta accuracies respectively.\n",
    "\n",
    "Fourth argument is the threshold, which means minimum vote it should get for it to be considered as a line.\n",
    "\n",
    "cv2.HoughLines(edges,1,np.pi/180,200)\n",
    "\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "**cv2.HoughLinesP()**\n",
    "\n",
    "is an optimization of Hough Transform\n",
    "\n",
    "returns the two endpoints of lines\n",
    "\n",
    "lines = cv2.HoughLinesP(E,rho = 1,theta = 1*np.pi/180,threshold = 100,minLineLength = 100,maxLineGap = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG6t__-4xHda"
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/vte15iohli93z5j/road.jpg?dl=0\" -O road.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4exEM2fxeMf"
   },
   "outputs": [],
   "source": [
    "img = read_and_resize_image(\"road.jpg\")\n",
    "showInRow([img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVA4YV2hxhKO"
   },
   "outputs": [],
   "source": [
    "def detectLines(max_slider):\n",
    "  global img\n",
    "  global dst\n",
    "  global gray\n",
    "\n",
    "  dst = np.copy(img)\n",
    "\n",
    "  th1 = 700\n",
    "  th2 = 200\n",
    "  edges = cv2.Canny(img, th1, th2)\n",
    "  showInRow([edges])\n",
    "\t# Apply probabilistic hough line transform\n",
    "  lines = cv2.HoughLinesP(edges, 2, np.pi/180.0, 50, minLineLength=10, maxLineGap=100)\n",
    "  # show_in_row([dst])\n",
    "\t# Draw lines on the detected points\n",
    "  for line in lines:\n",
    "    x1, y1, x2, y2 = line[0]\n",
    "    cv2.line(dst, (x1, y1), (x2, y2), (0,0,255), 1)\n",
    "\n",
    "\n",
    "\n",
    "# Create a copy for later usage\n",
    "dst = np.copy(img)\n",
    "\n",
    "# Convert image to gray\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize threshold value\n",
    "initThresh = 500\n",
    "\n",
    "# Maximum threshold value\n",
    "maxThresh = 1000\n",
    "\n",
    "# cv2.createTrackbar(\"threshold\", \"Result Image\", initThresh, maxThresh, onTrackbarChange)\n",
    "detectLines(initThresh)\n",
    "showInRow([dst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpHczm2IyMol"
   },
   "source": [
    "### **HoughCircles**\n",
    "\n",
    "The function HoughCircles is used in OpenCV to detect the circles in an image. It takes the following parameters:\n",
    "  \n",
    "  image: The input image.\n",
    "\n",
    "  method: Detection method, The available methods are HOUGH_GRADIENT and HOUGH_GRADIENT_ALT.\n",
    "\n",
    "  dp: the Inverse ratio of accumulator resolution and image resolution.\n",
    "\n",
    "  mindst: minimum distance between centers od detected circles.\n",
    "\n",
    "  param_1 and param_2: These are method specific parameters.\n",
    "\n",
    "  min_Radius: minimum radius of the circle to be detected.\n",
    "\n",
    "  max_Radius: maximum radius to be detected.\n",
    "\n",
    "https://docs.opencv.org/4.5.1/dd/d1a/group__imgproc__feature.html#ga47849c3be0d0406ad3ca45db65a25d2d\n",
    "\n",
    "HoughCircles function has **inbuilt** canny detection, therefore it is not required to detect edges explicitly in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DanGAnaryDJz"
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/tvqy4aq9ts9po04/eyes.jpg?dl=0\" -O eyes.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmsPBH2IyO-n"
   },
   "outputs": [],
   "source": [
    "def detectCircle(max_slider):\n",
    "    cimg = np.copy(img)\n",
    "\n",
    "    p1 = max_slider\n",
    "    p2 = max_slider * 0.4\n",
    "\n",
    "    # Detect circles using HoughCircles transform\n",
    "    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, cimg.shape[0]/64, param1=p1, param2=p2, minRadius=25, maxRadius=50)\n",
    "\n",
    "    # If at least 1 circle is detected\n",
    "    if circles is not None:\n",
    "        cir_len = circles.shape[1] # store length of circles found\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0, :]:\n",
    "            # Draw the outer circle\n",
    "            cv2.circle(cimg, (i[0], i[1]), i[2], (0, 255, 0), 2)\n",
    "    else:\n",
    "        cir_len = 0 # no circles detected\n",
    "\n",
    "    # Display output image\n",
    "    showInRow([cimg])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read image\n",
    "img = cv2.imread('eyes.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB )\n",
    "showInRow([img])\n",
    "# Convert to gray-scale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Trackbar will be used for changing threshold for edge\n",
    "initThresh = 105\n",
    "maxThresh = 200\n",
    "detectCircle(initThresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6skXBXeyelV"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YnKuLIxyT7j"
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/z40h2aeybcsznln/paper_test.jpg?dl=0\" -O paper_test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffKoiejYyhMR"
   },
   "outputs": [],
   "source": [
    "img = read_and_resize(\"paper_test.jpg\")\n",
    "showInRow([img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXpCks4fyjmM"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Find the paper's corners\n",
    "\n",
    "def find_paper_corners(color_img: np.array) -> np.array:\n",
    "  # return np.array with shape (4, 2)\n",
    "  # order: [top_left, top_right, bot_right, bot_left]\n",
    "\n",
    "  gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "  gray_img = cv2.GaussianBlur(gray_img, None, 1.0)\n",
    "  th, gray_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "  cnts, hierarchy = cv2.findContours(gray_img.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]\n",
    "\n",
    "  paper_contour = None\n",
    "  for c in cnts:\n",
    "      peri = cv2.arcLength(c, True)\n",
    "      approx = cv2.approxPolyDP(c, 0.1 * peri, True)\n",
    "\n",
    "      if len(approx) == 4:\n",
    "        paper_contour = approx\n",
    "        break\n",
    "\n",
    "  img_copy = color_img.copy()\n",
    "  if paper_contour is not None:\n",
    "    cv2.drawContours(img_copy, [paper_contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "  showInRow([gray_img, img_copy])\n",
    "  paper_contour = paper_contour.reshape((4, 2))\n",
    "  paper_contour = paper_contour[::-1]\n",
    "  paper_contour[:2], paper_contour[2:] = paper_contour[2:].copy(), paper_contour[:2].copy()\n",
    "  return paper_contour.reshape((4, 2))\n",
    "\n",
    "corners = find_paper_corners(img)\n",
    "print(corners)\n",
    "if corners.shape == (4, 2):\n",
    "  print(\"You are good\")\n",
    "  img_copy = img.copy()\n",
    "  cv2.polylines(img_copy, [corners], True, (255,0,0), 3)\n",
    "  showInRow([img_copy])\n",
    "else:\n",
    "  print(\"Shape must contain the answer to the ultimate question of life\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6Fim5Nyyn2C"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def undistort_perspective(img: np.array, corners: np.array) -> np.array:\n",
    "  dst_size = (1120, 1584)\n",
    "  dst_points = np.array([[0, 0], [dst_size[0], 0], [dst_size[0], dst_size[1]], [0, dst_size[1]]], np.int32)\n",
    "  h, status = cv2.findHomography(corners, dst_points)\n",
    "  return cv2.warpPerspective(img, h, dst_size)\n",
    "\n",
    "paper = undistort_perspective(img, corners)\n",
    "showInRow([paper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P8t3oqiyve3"
   },
   "outputs": [],
   "source": [
    "def find_answers(color_img: np.array) -> list:\n",
    "  # return in format [[A], [B, C], [], [A], ..]\n",
    "  gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "  # cv2.Canny()\n",
    "  dst = cv2.Canny(gray_img, 200, 200, None, 3)\n",
    "  # _, dst = cv2.threshold(gray_img, 160, 255, cv2.THRESH_BINARY_INV)\n",
    "  showInRow([dst])\n",
    "\n",
    "  lines = cv2.HoughLines(dst, 3, np.pi / 90, 500)\n",
    "  lines = sorted(lines, key=lambda x: x[0][0])\n",
    "  img_copy = color_img.copy()\n",
    "\n",
    "  i1, i2 = 0, 0\n",
    "  xs, ys = [], []\n",
    "  if lines is not None:\n",
    "    for i in range(len(lines)):\n",
    "      rho = lines[i][0][0]\n",
    "      theta = lines[i][0][1]\n",
    "      a = math.cos(theta)\n",
    "      b = math.sin(theta)\n",
    "      x0 = a * rho\n",
    "      y0 = b * rho\n",
    "      pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n",
    "      pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n",
    "      ang = theta * 180 / math.pi\n",
    "      if ang > 45:\n",
    "        i1 += 1\n",
    "        cv2.line(img_copy, pt1, pt2, (i1*30,0,255), 3, cv2.LINE_AA)\n",
    "        ys.append(pt1[1])\n",
    "      else:\n",
    "        i2 += 1\n",
    "        cv2.line(img_copy, pt1, pt2, (i2*60,255,0), 3, cv2.LINE_AA)\n",
    "        xs.append(pt1[0])\n",
    "  showInRow([color_img, img_copy])\n",
    "\n",
    "  answers = []\n",
    "  column_names = ['A', 'B', 'C', 'D']\n",
    "  thresh = 50\n",
    "  for row in range(len(ys)-1):\n",
    "    answers.append([])\n",
    "    for col in range(len(xs)-1):\n",
    "      content = dst[ys[row]+12:ys[row+1]-4, xs[col]+10:xs[col+1]-10]\n",
    "      if np.count_nonzero(content) > thresh:\n",
    "        answers[row].append(column_names[col])\n",
    "  return answers\n",
    "\n",
    "answers = find_answers(paper[150:-100,100:-100])\n",
    "print(answers)\n",
    "if len(answers) == 20 and answers[0] == ['C']:\n",
    "  print(\"Seems like you did well!\")\n",
    "else:\n",
    "  print(\"TODO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0yUlAYNy3xQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
